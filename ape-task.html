<HTML>
  <HEAD>
    <title>Automatic Post-Editing Task - ACL 2016 First Conference  on  Machine Translation</title>
    <style> h3 { margin-top: 2em; } </style>
  </HEAD>
  <body>

    <center>
      <script src="title.js"></script>
      <p><h2>Shared Task: Automatic Post-Editing</h2></p>
      <script src="menu.js"></script>
    </center>


<H3>OVERVIEW</H3>
<p> The third round of the APE shared task follows the success of the previous two rounds organised in 2015 and 2016. The aim is to examine <b> automatic methods for correcting errors produced by an unknown machine translation (MT) system.</b> This has to be done by exploiting knowledge acquired from human post-edits, which are provided as training material.</p>

<H3>Goals</H3>

<p>
The aim of this task is to improve MT output in black-box scenarios, in which the MT system is used "as is" and cannot be modified. From the application point of view, APE components would make it possible to:
<UL>
<LI>Cope with systematic errors of an MT system whose decoding process is not accessible</LI>
<LI>Provide professional translators with improved MT output quality to reduce (human) post-editing effort</LI>
<LI>Adapt the output of a general-purpose system to the lexicon/style requested in a specific application domain</LI>
</UL>
</p>

<H3>Task Description</H3> 
<p>
Similar to the last round, this year the task focuses on the Information Technology (IT) domain. One novelty, however, is represented by the addition of one language direction: this year, the task will hence cover English-German, and German-English. In both cases, the source sentences  have been translated into the target language by an MT system unknown to the participants and then manually post-edited by professional translators.</p>
<p>At training stage, the collected human post-edits have to be used to learn correction rules for the APE systems. At test stage they will be used for system evaluation with automatic metrics (TER and BLEU).
</p>

<H3>DIFFERENCES FROM THE SECOND ROUND (WMT 2016)</H3>
<p>
Compared to the the second round, the main differences are:
<UL>
<LI>Additional language direction (German-English);</LI>
<LI>Additional domain (Medical);</LI>
<LI>Larger data set.</LI>
</UL>
</p>

<H3>Data</H3>
<p>
Training, development and test data consist in English-German and German-English triplets (source, target, and post-edit) belonging to the IT and Medical domains respectively, and are <b>already tokenized.</b> All data is provided by the EU project QT21 (<a href="http://www.qt21.eu/" target="_blank">http://www.qt21.eu/</a>).</p>
<p>For EN-DE language direction, the development set released in 2016 can be used to tune the systems.</p>
</p>
<!-- <p>Training and development respectively contain 11K (EN-DE)/25K (De-EN) and 1,000 triplets, while the test set 2,000 instances. All data is provided by the EU project QT21 (<a href="http://www.qt21.eu/" target="_blank">http://www.qt21.eu/</a>).</p> -->

<p>To download the data click on the links in the table below:</p>
<table border=1 cellpadding=5>
<tr>
  <th>Language pair</th>
  <th>Domain</th>
  <th>2016</th>
  <th colspan="3">2017</th>
  <th>Additional Resource</th>
</tr>
<tr>
  <th>EN-DE</th>
  <th>IT</th>
  <th><a href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11372/LRT-1632" target="_blank" >train, dev, test<sup>*<sup></a></th>
  <th><a href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1966" target="_blank" >train</a></th>
  <th><a href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11372/LRT-2133" target="_blank" >test</a></th>
  <th><a href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11372/LRT-2483" target="_blank" >test post-edits</a></th>
  <th><a href="https://github.com/amunmt/amunmt/wiki/AmuNMT-for-Automatic-Post-Editing" target="_blank">artificial training data<sup>+</sup></a></th>
</tr>
<tr>
  <th>DE-EN</th>
  <th>Medical</th>
  <th></th>
  <th><a href="http://hdl.handle.net/11372/LRT-1967" target="_blank" >train, dev</a></th>
  <th><a href="http://hdl.handle.net/11372/LRT-2132" target="_blank" >test</a></th>
  <th><a href="http://lindat.mff.cuni.cz/repository/xmlui/handle/11372/LRT-2485" target="_blank" >test post-edits</a></th>
  <th></th>
</tr>
</table>
<p><sup>*</sup>: Test 2016 will be used as a progressive test set to measure the progress of the state-of-the-art systems.</p>
<p><sup>+</sup>: This training data was created and used in <a href="http://www.aclweb.org/anthology/W16-2378" target="_blank">&quot;Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing&quot;</a></p>
<p><b>NOTE:</b> <br>
1) Any use of additional data for training your system is allowed (e.g. parallel corpora, post-edited corpora).<br>
2) Please use the following citation, if you use these data sets in your publications.
<pre>
(TO BE ADDED)
<!--
@InProceedings{bojar-EtAl:2017:WMT1,
  author    = {Bojar, Ond\v{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and  Jimeno Yepes, Antonio  and  Koehn, Philipp  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Neveol, Aurelie  and  Neves, Mariana  and  Popel, Martin  and  Post, Matt  and  Rubino, Raphael  and  Scarton, Carolina  and  Specia, Lucia  and  Turchi, Marco  and  Verspoor, Karin  and  Zampieri, Marcos},
  title     = {Findings of the 2017 Conference on Machine Translation},
  booktitle = {Proceedings of the First Conference on Machine Translation},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {131--198},
  url       = {http://www.aclweb.org/anthology/W/W16/W16-2301}
}
-->
</pre>

<H3>Evaluation</H3>
<p>Systems' performance will be evaluated with respect to their capability to reduce the distance that separates an automatic translation from its human-revised version.</p>
<p>Such distance will be measured in terms of TER, which will be computed between automatic and human post-edits in <b>case-sensitive mode.</b></p>
<p>Also BLEU will be taken into consideration as a secondary evaluation metric. To gain further insights on final output quality, a subset of the outputs of the submitted systems will also be manually evaluated like in 2016.</p>
<p>The submitted runs will be ranked based on the average HTER calculated on the test set by using the <a href="http://www.cs.umd.edu/~snover/tercom/" target="_blank">tercom</a> software.</p>
<p>The HTER calculated between the raw MT output and human post-editions in the test set will be used as baseline (<i>i.e.</i> the baseline is a system that leaves all the test instances unmodified).</p>
<p>The evaluation script can be downloaded <a href="https://www.dropbox.com/s/5jw5maariwey080/Evaluation_Script.tar.gz?dl=0" target="_blank">here</a> </p>

<H3><font color="green">Results for EN-DE on 2017 test set</font></H3>
<table border="1" cellspacing="0" cellpadding="5">
<tr>
  <th>Systems</th>
  <th>TER</th>
  <th>BLEU</th>
</tr>
<tr>
  <td>FBK_EnsembleRerank_Primary</td>
  <td>19.6<sup>^</sup></td>
  <td>70.07<sup>^</sup></td>
</tr>
<tr>
  <td>AMU_multi-transducer-composed_PRIMARY</td>
  <td>19.77<sup>^</sup></td>
  <td>69.5<sup>^</sup></td>
</tr>
<tr>
  <td>AMU.multi-transducer.SECONDARY</td>
  <td>19.83<sup>^</sup></td>
  <td>69.38<sup>^</sup></td>
</tr>
<tr>
  <td>DCU_FRANKENAPE-TUNED_PRIMARY</td>
  <td>20.11<sup>^</sup></td>
  <td>69.19<sup>^</sup></td>
</tr>
<tr>
  <td>DCU_FRANKENAPE-TUNED_CONTRASTIVE</td>
  <td>20.25<sup>^</sup></td>
  <td>69.33<sup>^</sup></td>
</tr>
<tr>
  <td>FBK_SingleModelRerank_Contrastive</td>
  <td>20.3<sup>^</sup></td>
  <td>69.11<sup>^</sup></td>
</tr>
<tr>
  <td>FBK_USAARRerankStatFeat_Contrastive</td>
  <td>21.55<sup>^</sup></td>
  <td>67.28<sup>^</sup></td>
</tr>
<tr>
  <td>USAAR_NMT-OSM_PRIMARY</td>
  <td>23.05<sup>^</sup></td>
  <td>65.01<sup>^</sup></td>
</tr>
<tr>
  <td>LIG_chained_syn_PRIMARY</td>
  <td>23.22<sup>^</sup></td>
  <td>65.12<sup>^</sup></td>
</tr>
<tr>
  <td>JXNU_JXNU_EDITFreq_PRIMARY</td>
  <td>23.31<sup>^</sup></td>
  <td>65.66<sup>^</sup></td>
</tr>
<tr>
  <td>LIG_forced_CONTRASTIVE</td>
  <td>23.51<sup>^</sup></td>
  <td>64.52<sup>^</sup></td>
</tr>
<tr>
  <td>LIG_chained_CONTRASTIVE</td>
  <td>23.66<sup>^</sup></td>
  <td>64.46<sup>^</sup></td>
</tr>
<tr>
  <td>CUNI_char_conv_rnn_beam_PRIMARY</td>
  <td>24.03</td>
  <td>64.28<sup>^</sup></td>
</tr>
<tr>
  <td>USAAR_OSM_CONTRASTIVE</td>
  <td>24.17<sup>^</sup></td>
  <td>63.55<sup>^</sup></td>
</tr>
<tr bgcolor="#DCDCDC">
  <td>Official Baseline (MT)</td>
  <td>24.48</td>
  <td>62.49</td>
</tr>
<tr bgcolor="#DCDCDC">
  <td>Baseline_2 (Statistical phrase-based APE)</td>
  <td>24.69<sup>^</sup></td>
  <td>62.97<sup>^</sup></td>
</tr>
<tr>
  <td>CUNI_char_conv_rnn_greedy_CONTRASTIVE</td>
  <td>25.94<sup>^</sup></td>
  <td>61.65<sup>^</sup></td>
</tr>
</table>
<sup>^</sup>: indicates the score is statistically significant wrt. official baseline (MT)

<H3><font color="green">Results for DE-EN on 2017 test set</font></H3>
<table border="1" cellspacing="0" cellpadding="5">
<tr>
  <th>Systems</th>
  <th>TER</th>
  <th>BLEU</th>
</tr>
<tr>
  <td>FBK_EnsembleRerank_Primary</td>
  <td>15.29<sup>^</sup></td>
  <td>79.82<sup>^</sup></td>
</tr>
<tr>
  <td>FBK_SingleModelRerank_Contrastive</td>
  <td>15.31<sup>^</sup></td>
  <td>79.64</td>
</tr>
<tr>
  <td>LIG_chained_syn_PRIMARY</td>
  <td>15.53</td>
  <td>79.49</td>
</tr>
<tr bgcolor="#DCDCDC">
  <td>Official Baseline (MT)</td>
  <td>15.55</td>
  <td>79.54</td>
</tr>
<tr>
  <td>LIG_forced_CONTRASTIVE</td>
  <td>15.62</td>
  <td>79.48</td>
</tr>
<tr>
  <td>LIG_chained_CONTRASTIVE</td>
  <td>15.68<sup>^</sup></td>
  <td>79.35<sup>^</sup></td>
</tr>
<tr bgcolor="#DCDCDC">
  <td>Baseline_2 (Statistical phrase-based APE)</td>
  <td>15.74<sup>^</sup></td>
  <td>79.28</td>
</tr>
</table>

<H3>Submission Format</H3>

<p>
The output of your system should produce automatic post-editions of the target sentences in the test in the following way:
<pre>
<b>&lt;METHOD NAME&gt;   &lt;SEGMENT NUMBER&gt;   &lt;APE SEGMENT&gt;</b>
</pre>
</p>

Where:
<ul>
<li><code><b>METHOD NAME</b></code> is the name of your automatic post-editing method.</li>
<li><code><b>SEGMENT NUMBER</b></code> is the line number of the plain text target file you are post-editing.</li>
<li><code><b>APE SEGMENT</b></code> is the automatic post-edition for the particular segment.</li>
</ul>
Each field should be delimited by a single tab character.
</p>

<H3>Submission Requirements</H3>

<p>Each participating team can submit at most 3 systems, but they have to explicitly indicate which of them represents their <i>primary</i> submission. In the case that none of the runs is marked as primary, the latest submission received will be used as the primary submission.</p>

 
<p>Submissions should be sent via email to <font color="red"><a href="mailto:wmt-ape-submission@fbk.eu">wmt-ape-submission@fbk.eu</a></font>. Please use the following pattern to name your files:</p>

<p><code><b>INSTITUTION-NAME_METHOD-NAME_SUBTYPE</b></code>, where:</p>

<p><code><b>INSTITUTION-NAME</b></code> is an acronym/short name for your institution, e.g. "UniXY"</p>

<p><code><b>METHOD-NAME</b></code> is an identifier for your method, e.g. "pt_1_pruned"</p>

<p><code><b>SUBTYPE</b></code> indicates whether the submission is primary or contrastive with the two alternative values: <code>PRIMARY</code>, <code>CONTRASTIVE</code>.</p>

<p>You are also invited to submit a short paper (4 to 6 pages) to WMT describing your APE method(s). You are not required to submit a paper if you do not want to. In that case, we ask you to give an appropriate reference describing your method(s) that we can cite in the WMT overview paper.</p>

<h3>Important dates</h3>

    <table>
      <tr><td>Release of training data </td><td>February 16, 2017</td></tr>
      <tr><td>Release of test data  </td><td>April 10 2017</td></tr>
      <tr><td>Submission deadline  </td><td><strike>May 6 2017</strike> May 13 2017</td></tr>
      <tr><td>Paper submission deadline</td><td><strike>June 2 2017</strike> June 9 2017</td></tr>
      <tr><td>Manual evaluation</td><td>TBD</td></tr>
      <tr><td>Notification of acceptance</td><td>June 30 2017</td></tr>
      <tr><td>Camera-ready deadline</td><td>July 14 2017</td></tr>
    </table>

<h3>Organisers</h3>
Rajen Chatterjee (Fondazione Bruno Kessler)
<br>
Yvette Graham (Dublin City University)
<br>
Matteo Negri (Fondazione Bruno Kessler)
<br>
Raphael Rubino (Saarland University)
<br>
Marco Turchi (Fondazione Bruno Kessler)

<h3>Contact</h3>
<p>For any information or question on the task, please send an email to:<a href="mailto:wmt-ape@fbk.eu">wmt-ape@fbk.eu</a>.<br> 
To be always updated about this year's edition of the APE task, you can also join the <a href="http://groups.google.com/a/fbk.eu/group/wmt-ape/" target="_blank">wmt-ape group</a>.</p>

<p align="right">
Supported by the European Commission under the QT21
<a href="http://www.qt21.eu/"><img align=right src="figures/qt21.png" border=0 width=100 height=40></a>
<br>project (grant number 645452) <p>

</HTML>
