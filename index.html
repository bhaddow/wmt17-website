<html>
  <head>
    <title>2017 Second Conference  on  Machine Translation (WMT17)</title>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <style> h3 { margin-top: 2em; } </style>
  </head>
  <body>

    <center>
      <script src="title.js"></script>
      <p><h2>Home</h2></p>
      <script src="menu.js"></script>
    </center>

   <p>

      This conference builds on a series of annual workshops and conferences on statistical machine
      translation, going back to 2006:

      <UL id="">
        <LI>the <a href="http://www.statmt.org/wmt06/">NAACL-2006 Workshop on
            Statistical Machine Translation</a>,
        <LI>the <a href="http://www.statmt.org/wmt07/">ACL-2007 Workshop on Statistical
            Machine Translation</a>, 
        <LI>the <a href="http://www.statmt.org/wmt08/">ACL-2008
            Workshop on Statistical Machine Translation</a>, 
        <LI> the <a href="http://www.statmt.org/wmt09/">EACL-2009 Workshop on
            Statistical Machine Translation</a>,
        <LI> the <a href="http://www.statmt.org/wmt10/">ACL-2010 Workshop on
            Statistical Machine Translation</a>
        <LI> the <a href="http://www.statmt.org/wmt11/">EMNLP-2011 Workshop on
            Statistical Machine Translation</a>,
        <LI> the <a href="http://www.statmt.org/wmt12/">NAACL-2012 Workshop on
            Statistical Machine Translation</a>, 
        <LI> the <a href="http://www.statmt.org/wmt13/">ACL-2013 Workshop on
            Statistical Machine Translation</a>,
        <LI> the <a href="http://www.statmt.org/wmt14/">ACL-2014 Workshop on
            Statistical Machine Translation</a>, 
        <LI> the <a href="http://www.statmt.org/wmt15/">EMNLP-2015 Workshop on
            Statistical Machine Translation</a>,
        <LI> the <a href="http://www.statmt.org/wmt16/">First Conference on
            Machine Translation (at ACL-2016)</a>.
      </UL>
    </p>
 
    <h3>IMPORTANT DATES</h3>
    <table>
      <tr><td>Release of training data for shared tasks</td><td>January/February, 2017</td></tr>
      <tr><td>Evaluation periods for shared tasks</td><td>April/May, 2017</td></tr>
      <tr><td>Paper submission deadline</td><td>June 9th, 2017 (Midnight, UTC -11)</td></tr>
      <tr><td>Paper notification</td><td>June 30th, 2017</td></tr>
      <tr><td>Camera-ready version due</td><td>July 14th, 2017</td></tr>
      <tr><td>Conference in Copenhagen</td><td>September 7-8, 2017</td></tr>
    </table>

    <h3>OVERVIEW</h3>
    <p>
      This year's conference will feature the following shared tasks: 
<ul><li>a news  translation task,
    <li>a biomedical translation task ,
    <li>an  automatic post-editing task,
    <li>a metrics task (assess MT quality given reference translation).
    <li>a quality estimation task (assess MT quality without access to any reference),
    <li>a multimodal translation task
    <li>a task dedicated to the training of neural MT systems
    <li>a task on bandit learning for MT
</ul>
    </p>

    <p>
      In addition to the shared tasks, the conference will also feature scientific papers on topics related to MT.
      Topics of interest include, but are not limited to:
      <ul>
        <li> word-based, phrase-based, syntax-based, semantics-based SMT</li>
        <li> neural machine translation</li>
        <li> using comparable corpora for SMT</li>
        <li> incorporating linguistic information into SMT</li>
        <li> decoding</li>
        <li> system combination</li>
        <li> error analysis</li>
        <li> manual and automatic method for evaluating MT</li>
        <li> scaling MT to very large data sets</li>
      </ul>
      We encourage authors to evaluate their approaches to the above topics
      using the common data sets created for the shared tasks.
    </p>

    <h3>REGISTRATION</h3>
    Registration will be handled by <a href="http://emnlp2017.net/">EMNLP 2017</a>.

    <h3>NEWS TRANSLATION TASK</h3>
    <p>
      The first shared task which will examine translation between the
      following language pairs:
      <ul>
	      <li> English-Chinese and Chinese-English <font color=red><b>NEW</b></font> </li>
        <li> English-Czech and Czech-English</li>
        <li> English-Finnish and Finnish-English </li>
        <li> English-German and German-English</li>
	      <li> English-Latvian and Latvian-English <font color=red><b>NEW</b></font></li>
	      <li> English-Russian and Russian-English </li>
	      <li> English-Turkish and Turkish-English </li>
      </ul>
      The text for all the test sets will be drawn from news articles.
      Participants may submit translations for any or all of the language
      directions. In addition to the common test sets the conference organizers
      will provide optional training resources.
    </p>

    <p>
      All participants who submit entries will have their translations
      evaluated. We will evaluate translation performance by human judgment. To
      facilitate the human evaluation we will require participants in the
      shared tasks to manually judge some of the submitted translations. For each team,
      this will amount
      to ranking 300 sets of 5 translations, per language pair submitted.
    </p>

<h3>BIOMEDICAL TRANSLATION TASK</h3> 

<p>In this second edition of this task, we will evaluate systems for the translation of biomedical documents for the following languages pairs:<p>

<ul>
<li> English-Czech and Czech-English <font color=red><b>NEW</b></font></li>
<li> English-French and French-English </li>
<li> English-German and German-English <font color=red><b>NEW</b></font></li>
<li> English-Hungarian and Hungarian-English <font color=red><b>NEW</b></font></li>
<li> English-Polish and Polish-English <font color=red><b>NEW</b></font></li>
<li> English-Portuguese and Portuguese-English </li>
<li> English-Romanian and Romanian-English <font color=red><b>NEW</b></font></li>
<li> English-Spanish and Spanish-English </li>
<li> English-Swedish and Swedish-English <font color=red><b>NEW</b></font></li>
</ul>

<p>Parallel corpora will be available for all language pairs but also monoligual corpora for some languages. 
Evaluation will be carried out both automatically and manually.</p>



	<h3>AUTOMATIC POST-EDITING TASK</h3>
<p>This shared task will examine automatic methods for correcting errors produced by machine translation (MT) systems.  Automatic Post-editing (APE) aims at improving MT output in black box scenarios, in which the MT system is used "as is" and cannot be modified.
From the application point of view APE components would make it possible to:</p>
<ul>
<li> Cope with systematic errors of an MT system whose decoding process is not accessible </li>
<li> Provide professional translators with improved MT output quality to reduce (human) post-editing effort </li>
</ul>

<p>In this third edition of the task, the evaluation will focus on English-German (IT domain) and German-English (Medical domain)</p>


    <h3>METRICS TASK</h3>
<p>The metrics task (also called evaluation task) will assess automatic evaluation metrics' ability to:
</p>

<ul>
<li> Evaluate systems on their overall performance on the test set </li>
<li> Evaluate systems on a sentence by sentence level </li>
</ul>

<p>
Participants in the shared evaluation task will use their automatic evaluation metrics to score the output from the translation task and the NMT training task.  In addition to MT outputs from the other two tasks, the participants will be provided with reference translations.   We will measure the correlation of automatic evaluation metrics with the human judgments.  
</p>

<!--
    <h3>QUALITY ESTIMATION TASK</h3>

<p> Quality estimation systems aim at producing an estimate on the quality of a given translation at system run-time, without access to a reference translation. This topic is particularly relevant from a user perspective. Among other applications, it can (i) help decide whether a given translation is good enough for publishing as is; (ii) filter out sentences that are not good enough for post-editing; (iii) select the best translation among options from multiple MT and/or translation memory systems; (iv) inform readers of the target language of whether or not they can rely on a translation; and (v) spot parts (words or phrases) of a translation that are potentially incorrect.</p>

    <p>Research on this topic has been showing promising results in the last couple of years. Building on the last three years' experience, the Quality-Estimation track of the WMT15 workshop and shared-task will focus on English, Spanish and German as languages and provide new training and test sets, along with evaluation metrics and baseline systems for variants of the task at three different levels of prediction: word, sentence, and document.

-->
    <h3>NEURAL MT TRAINING TASK (NMT TRAINING TASK)</h3>

This task will assess your team's ability to <b>train a fixed neural MT model</b> given fixed data.

<p>
Participants in the NMT training task will be given complete a Neural Monkey configuration file which describes the neural model. Training and validation data with fixed pre-processing scheme will be also given (English-to-Czech and Czech-to-English translation).
</p>
<p>
The participants will be expected to submit the variables file, i.e. the trained neural network, for one or both of the translation directions. We will use the variables and a fixed revision of Neural Monkey to translate official WMT17 test set. The outputs of the various configurations of the system will be scored using the standard manual evaluation procedure.
</p>

<h3>BANDIT LEARNING TASK</h3>
<p>Bandit Learning for MT is a framework to train and improve MT systems by learning from weak or partial feedback: Instead of a gold-standard human-generated translation, the learner only receives feedback to a single proposed translation (this is why it is called partial), in form of a translation quality judgement (which can be as weak as a binary acceptance/rejection decision).
</p>
<!--<p> Amazon and University of Heidelberg organize this Shared Task with a goal to encourage researchers to investigate algorithms for learning from weak user feedback instead of from human references or post-edits that require skilled translators. We are interested in finding systems that learn efficiently and effectively from this type of feedback, i.e. they learn fast and achieve high translation quality. Developing such algorithms is interesting for interactive machine learning and for human feedback in NLP in general.
</p>-->
<p> In this task, the user feedback will be simulated by a service hosted on Amazon Web Services (AWS), where participants can submit translations and receive feedback and use this feedback for training an MT model (German-to-English, e-commerce). Reference translations will not be revealed at any point, also evaluations are done via the service. The goal of this task is to find systems that learn efficiently and effectively from this type of feedback, i.e. they learn fast and achieve high translation quality without references.
</p>

<!--
<h3>CROSS-LINGUAL PRONOUN PREDICTION TASK</h3>
<p>Pronoun translation poses a problem for current state-of-the-art SMT systems as pronoun systems do not map well across languages, e.g., due to differences in gender, number, case, formality, or humanness, and to differences in where pronouns may be used. Translation divergences typically lead to mistakes in SMT, as when translating the English "it" into French ("il", "elle", or "cela"?) or into German ("er", "sie", or "es"?). One way to model pronoun translation is to treat it as a cross-lingual pronoun prediction task.</p>

    <p>We propose such a task, which asks participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provide a lemmatised target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the target-language lemmata. In the translation, the words aligned to a subset of the source-language third-person pronouns are substituted by placeholders. The aim of the task is to predict, for each placeholder, the word that should replace it from a small, closed set of classes, using any type of information that can be extracted from the documents.</p>

    <p>The cross-lingual pronoun prediction task will be similar to the task of the same name at DiscoMT 2015:</p>

	<a href="http://www.idiap.ch/workshop/DiscoMT/shared-task">http://www.idiap.ch/workshop/DiscoMT/shared-task</a>

    <p>Participants are invited to submit systems for the English-French and English-German language pairs, for both directions.</p>

<h3>BILINGUAL DOCUMENT ALIGNMENT TASK</h3>
Details TBC

<h3>MULTIMODAL TRANSLATION TASK</h3>

This is a new task where participants are requested to generate a description for an image in a target language, given the image itself and one or more descriptions in a different (source) language. 

-->

    <h3>PAPER SUBMISSION INFORMATION</h3>

    <p>
      Submissions will consist of regular full papers of 6-10 pages, plus
      additional pages for references, formatted following the 
      <a href="http://emnlp2017.net/call-for-papers.html">EMNLP 2017
      guidelines</a>. In addition, shared task participants will be invited to
      submit short papers (suggested length: 4-6 pages, plus references) describing their systems or their
      evaluation metrics. Both submission and review processes will be handled
      <a href="https://www.softconf.com/emnlp2017/wmt/">electronically</a>.
      Note that regular papers must be anonymized, while system descriptions 
      do not need to be.
    </p>

    <p>
    Research papers that have been or will be submitted to other meetings or publications must indicate this at submission time, and must be withdrawn from the other venues if accepted and  published at WMT 2017. 
    We will not accept for publication papers that overlap significantly in content or results with papers that have been or will be published elsewhere.
    It is acceptable to submit work that has been made available as a technical report (or similar, e.g. in arXiv) without citing it.
    This double submission policy only applies to research papers, so system papers can have significant overlap with other published work, if it is relevant to the system description.
    </p>

    <p>
      We encourage individuals who are submitting research papers to evaluate
      their approaches using the training resources provided by this conference
      and past workshops, so that their experiments can be repeated by others
      using these publicly available corpora.
    </p>

<h3>POSTER FORMAT</h3>

A0 Landscape.

<h3>ANNOUNCEMENTS</h3>

<table border=0 cellspacing=0>
  <tr><td style="padding-left: 5px">
  Subscribe to to the announcement list for WMT by entering your e-mail address below.  This list will be used to announce when the test sets are released, to indicate any corrections to the training sets, and to amend the deadlines as needed.  
  </td></tr>
  <form action="http://groups.google.com/group/wmt-tasks/boxsubscribe">
  <tr><td style="padding-left: 5px;">
  Email: <input type=text name=email>
  <input type=submit name="sub" value="Subscribe">
  </td></tr>
</form>
<tr>
<td>
</table>

<p>
<table border=0 cellspacing=0>
  <tr><td>
You can read <a href="http://groups.google.com/group/wmt-tasks">past
	announcements</a> on the Google Groups page for WMT.  These also
include an archive of announcements from earlier workshops.</td><td  style="padding-left: 25px" align=right nowrap>
  <a href="http://groups.google.com/group/wmt-tasks"><img src="http://groups.google.com/groups/img/3nb/groups_bar.gif"
	  height=26 width=132 alt="Google Groups"></a>
</td></tr>
</table>
</p>

<h3>INVITED TALK</h3>
Holger Schwenk (Facebook)<br>
<i>Multilingual Representions and Applications in NLP</i><br>

<h3>ORGANIZERS</h3>
Ondřej Bojar (Charles University in Prague)<br>
Christian Buck (University of Edinburgh)<br>
Rajen Chatterjee (FBK)<br>
Christian Federmann (MSR)<br>
Yvette Graham (DCU)<br>
Barry Haddow (University of Edinburgh)<br>
Matthias Huck (University of Edinburgh)<br>
Antonio Jimeno Yepes (IBM Research Australia)<br>
Philipp Koehn (University of Edinburgh / Johns Hopkins University)<br>
Julia Kreutzer (Heidelberg University)<br>
Varvara Logacheva (University of Sheffield)<br>
Christof Monz (University of Amsterdam)<br>
Matteo Negri (FBK)<br>
Aur&eacute;lie N&eacute;v&eacute;ol (LIMSI, CNRS)<br>
Mariana Neves (Federal Institute for Risk Assessment / Hasso Plattner Institute)<br>
Matt Post (Johns Hopkins University)<br>
Stefan Riezler (Heidelberg University)<br>
<!--Carolina Scarton (University of Sheffield)<br>-->
Artem Sokolov (Heidelberg University, Amazon Development Center, Berlin)<br>
Lucia Specia (University of Sheffield)<br>
Marco Turchi (FBK)<br>
Karin Verspoor (University of Melbourne)<br>

<h3>PROGRAM COMMITTEE</h3>
<ul>
<li> Tim Anderson (Air Force Research Laboratory) </li>
<li> Eleftherios Avramidis (German Research Center for Artificial Intelligence (DFKI)) </li>
<li> Daniel Beck (University of Melbourne) </li>
<li> Arianna Bisazza (University of Amsterdam) </li>
<li> Graeme Blackwood (IBM Research) </li>
<li> Fr&eacute;d&eacute;ric Blain (University of Sheffield) </li>
<li> Ozan Caglayan (LIUM, Le Mans University) </li>
<li> Marine Carpuat (University of Maryland) </li>
<li> Francisco Casacuberta (Universitat Polit&egrave;cnica de Val&egrave;ncia) </li>
<li> Daniel Cer (Google) </li>
<li> Mauro Cettolo (FBK) </li>
<li> Rajen Chatterjee (Fondazione Bruno Kessler) </li>
<li> Boxing Chen (NRC) </li>
<li> Colin Cherry (NRC) </li>
<li> David Chiang (University of Notre Dame) </li>
<li> Eunah Cho (Karlsruhe Institute of Technology) </li>
<li> Kyunghyun Cho (New York University) </li>
<li> Vishal Chowdhary (MSR) </li>
<li> Jonathan Clark (Microsoft) </li>
<li> Marta R. Costa-juss&agrave; (Universitat Polit&egrave;cnica de Catalunya) </li>
<li> Praveen Dakwale (University of Amsterdam) </li>
<li> Steve DeNeefe (SDL Language Weaver) </li>
<li> Michael Denkowski (Amazon.com, Inc.) </li>
<li> Markus Dreyer (Amazon.com) </li>
<li> Nadir Durrani (QCRI) </li>
<li> Desmond Elliott (University of Edinburgh) </li>
<li> Marzieh Fadaee (University of Amsterdam) </li>
<li> Marcello Federico (FBK) </li>
<li> Minwei Feng (IBM Watson Group) </li>
<li> Yang Feng (Institute of Computing Technology, Chinese Academy of Sciences) </li>
<li> Andrew Finch (NICT) </li>
<li> Orhan Firat (Google Research) </li>
<li> Marina Fomicheva (Universitat Pompeu Fabra) </li>
<li> Jos&eacute; A. R. Fonollosa (Universitat Polit&egrave;cnica de Catalunya) </li>
<li> Mikel L. Forcada (Universitat d'Alacant) </li>
<li> George Foster (National Research Council) </li>
<li> Alexander Fraser (Ludwig-Maximilians-Universit&auml;t M&uuml;nchen) </li>
<li> Markus Freitag (IBM Research) </li>
<li> Ekaterina Garmash (University of Amsterdam) </li>
<li> Ulrich Germann (University of Edinburgh) </li>
<li> Hamidreza Ghader (Informatics Institute, University of Amsterdam) </li>
<li> Jes&uacute;s Gonz&aacute;lez-Rubio (Universitat Polit&egrave;cnica de Val&egrave;ncia) </li>
<li> Cyril Goutte (National Research Council Canada) </li>
<li> Thanh-Le Ha (Karlsruhe Institute of Technology) </li>
<li> Nizar Habash (New York University Abu Dhabi) </li>
<li> Jan Hajic (Charles University) </li>
<li> Greg Hanneman (Carnegie Mellon University) </li>
<li> Christian Hardmeier (Uppsala universitet) </li>
<li> Eva Hasler (SDL) </li>
<li> Yifan He (Bosch Research and Technology Center) </li>
<li> Kenneth Heafield (University of Edinburgh) </li>
<li> Carmen Heger (Iconic) </li>
<li> John Henderson (MITRE) </li>
<li> Felix Hieber (Amazon Research) </li>
<li> St&eacute;phane Huet (Universit&eacute; d'Avignon) </li>
<li> Young-Sook Hwang (SKPlanet) </li>
<li> Gonzalo Iglesias (SDL) </li>
<li> Doug Jones (MIT Lincoln Laboratory) </li>
<li> Marcin Junczys-Dowmunt (Adam Mickiewicz University, Pozna&#324;) </li>
<li> Roland Kuhn (National Research Council of Canada) </li>
<li> Shankar Kumar (Google) </li>
<li> &Aacute;kos K&aacute;d&aacute;r (Tilburg University) </li>
<li> David Langlois (LORIA, Universit&eacute; de Lorraine) </li>
<li> William Lewis (Microsoft Research) </li>
<li> Qun Liu (Dublin City University) </li>
<li> Shujie Liu (Microsoft Research Asia, Beijing, China) </li>
<li> Saab Mansour (Apple) </li>
<li> Daniel Marcu (ISI/USC) </li>
<li> Arne Mauser (Google, Inc) </li>
<li> Mohammed Mediani (Karlsruhe Institute of Technology) </li>
<li> Abhijit Mishra (IBM Research India) </li>
<li> Maria Nadejde (University of Edinburgh) </li>
<li> Preslav Nakov (Qatar Computing Research Institute, HBKU) </li>
<li> Jan Niehues (Karlsruhe Institute of Technology) </li>
<li> Kemal Oflazer (Carnegie Mellon University - Qatar) </li>
<li> Tsuyoshi Okita (Kyushuu institute of technology university) </li>
<li> Daniel Ortiz-Mart&iacute;nez (Technical University of Valencia) </li>
<li> Martha Palmer (University of Colorado) </li>
<li> Siddharth Patwardhan (IBM Watson) </li>
<li> Pavel Pecina (Charles University) </li>
<li> Stephan Peitz (Apple) </li>
<li> Sergio Penkale (Lingo24) </li>
<li> Jan-Thorsten Peter (RWTH Aachen University) </li>
<li> Maja Popovi&#263; (Humboldt University of Berlin) </li>
<li> Preethi Raghavan (IBM Research TJ Watson) </li>
<li> Stefan Riezler (Heidelberg University) </li>
<li> Baskaran Sankaran (IBM T.J. Watson Research Center) </li>
<li> Jean Senellart (SYSTRAN) </li>
<li> Rico Sennrich (University of Edinburgh) </li>
<li> Wade Shen (MIT) </li>
<li> Michel Simard (NRC) </li>
<li> Patrick Simianer (Heidelberg University) </li>
<li> Linfeng Song (University of Rochester) </li>
<li> Sara Stymne (Uppsala University) </li>
<li> Katsuhito Sudoh (Nara Institute of Science and Technology (NAIST)) </li>
<li> Felipe S&aacute;nchez-Mart&iacute;nez (Universitat d'Alacant) </li>
<li> Ale&scaron; Tamchyna (Charles University in Prague, UFAL MFF) </li>
<li> J&ouml;rg Tiedemann (University of Helsinki) </li>
<li> Christoph Tillmann (IBM Research) </li>
<li> Ke M. Tran (University of Amsterdam) </li>
<li> Dan Tufi&#537; (Research Institute for Artificial Intelligence, Romanian Academy) </li>
<li> Marco Turchi (Fondazione Bruno Kessler) </li>
<li> Ferhan Ture (Comcast Labs) </li>
<li> Masao Utiyama (NICT) </li>
<li> David Vilar (Amazon) </li>
<li> Stephan Vogel (Qatar Computing Research Institute) </li>
<li> Martin Volk (University of Zurich) </li>
<li> Taro Watanabe (Google) </li>
<li> Bonnie Webber (University of Edinburgh) </li>
<li> Marion Weller-Di Marco (LMU M&uuml;nchen, Universit&auml;t Stuttgart) </li>
<li> Philip Williams (University of Edinburgh) </li>
<li> Hua Wu (Baidu) </li>
<li> Joern Wuebker (Lilt, Inc.) </li>
<li> Fran&ccedil;ois Yvon (LIMSI/CNRS) </li>
<li> Marlies van der Wees (University of Amsterdam) </li>
</ul>

    <H3>ANTI-HARASSMENT POLICY</H3>

    <p>
      WMT follows the ACL's <a href="https://www.aclweb.org/adminwiki/index.php?title=Anti-Harassment_Policy">anti-harassment policy</a>
    </P>

    <h3>CONTACT</h3>

    <p>
      For general questions, comments, etc. please send email
      to <a href="mailto:bhaddow@inf.ed.ac.uk">bhaddow@inf.ed.ac.uk</a>.<br>
      For task-specific questions, please contact the relevant organisers.
    </p>

<h3>ACKNOWLEDGEMENTS</h3>
This conference has received funding
from the European Union’s Horizon 2020 research
and innovation programme under grant agreements
645452 (<a href="http://www.qt21.eu/">QT21</a>) and  645357 (<a href="http://www.meta-net.eu/projects/cracker/">Cracker</a>).
<br>
We thank <a href="http://www.yandex.com">Yandex</a> for their donation of data for the Russian-English and Turkish-English news tasks, and the University of Helsinki for their donation for the Finnish-English news tasks.

<!-- Supported by the European Commision<br> under the
<a href="http://www.mosescore.eu"><img align=right src="http://www.statmt.org/mosescore/pub/img/mosescore-logo-transp.png" border=0 width=100 height=20></a>  <a href="http://www.mosescore.eu/">MosesCore</a> project <br>project (grant number 288487) <p>
-->
  </body>
</html>
