
<html>
  <HEAD>
    <title>Multimodal Translation Task - 2017 Second Conference on Machine Translation (WMT17)</title>
<style type="text/css">
div.results {
  width: 70%;
}

table.results {
  width: 70%;
}

tr.results {
  vertical-align: top;
}

div.table {
  text-align: center;
}

tr.data-header {
  font-size: 16px;
}

td.data-position {
  font-size: 12px;
  text-align: right;
  width: 10px;
}

table.google-visualization-table-table {
	  font-size: 14px;
}
</style>
  </HEAD>
  <body>

    <center>
      <script src="title.js"></script>
      <p><h2>Shared Task: Multimodal Machine Translation</h2></p>
      <script src="menu.js"></script>
    </center>

<script src="https://www.google.com/jsapi?.js"></script>
<script type="text/javascript">

function return_data() {
	  // En-De Flickr 2017
	  var data = google.visualization.arrayToDataTable([
			      ['Submission Name', 'BLEU','Meteor', 'TER'],
			         ['AFRL_OSU-MULTIMODAL',6.5,20.2,87.4],
				 ['Baseline',19.3,41.9,72.2],
				 ['CUNI_NeuralMonkeyMultimodalMT_C',25.8,47.1,56.3],
				 ['CUNI_NeuralMonkeyMultimodalMT_U',29.5,50.2,52.5],
				 ['CUNI_NeuralMonkeyTextualMT_C',28.5,49.2,54.3],
				 ['CUNI_NeuralMonkeyTextualMT_U',31.1,51.0,50.7],
				 ['DCU-ADAPT_MultiMT_C',29.8,50.5,52.3],
				 ['LIUMCVC_MNMT_C',33.4,54.0,48.5],
				 ['LIUMCVC_NMT_C',33.2,53.8,48.2],
				 ['NICT_NMTrerank_C',31.9,53.9,48.1],
				 ['OSU_1NeuralTranslation_C',29.7,48.9,51.6],
				 ['OSU_2NeuralTranslation_C',31.0,50.6,50.7],
				 ['SHEF_ShefClassInitDec_C',25.0,44.5,53.8],
				 ['SHEF_ShefClassProj_C',24.2,43.4,55.9],
				 ['UvA-TiCC_IMAGINATION_C',30.2,51.2,50.8],
				 ['UvA-TiCC_IMAGINATION_U',33.3,53.5,47.5],
				   ]);

	  // En-De COCO 2017
	  var data2 = google.visualization.arrayToDataTable([
			      ['Submission Name', 'BLEU','Meteor', 'TER'],
			      ['Baseline',18.7,37.6,66.1],
			      ['CUNI_NeuralMonkeyMultimodalMT_C',22.4,42.7,60.1],
			      ['CUNI_NeuralMonkeyMultimodalMT_U',25.7,45.6,55.7],
			      ['CUNI_NeuralMonkeyTextualMT_C',23.2,43.8,59.8],
			      ['CUNI_NeuralMonkeyTextualMT_U',26.6,46.0,54.8],
			      ['DCU-ADAPT_MultiMT_C',26.4,46.8,54.5],
			      ['LIUMCVC_MNMT_C',28.5,48.8,53.4],
			      ['LIUMCVC_NMT_C',28.7,48.9,52.5],
			      ['NICT_1_NMTrerank_C',28.1,48.5,52.9],
			      ['OSU_1NeuralTranslation_C',27.4,46.5,52.3],
			      ['OSU_2NeuralTranslation_C',26.1,45.7,55.9],
			      ['SHEF_ShefClassInitDec_C',21.4,40.7,56.5],
			      ['SHEF_ShefClassProj_C',21.0,40.0,57.8],
			      ['UvA-TiCC_IMAGINATION_C',26.4,45.8,55.4],
			      ['UvA-TiCC_IMAGINATION_U',28.0,48.1,52.4]
				   ]);

	  // En-Fr Flickr 2017
	  var data3 = google.visualization.arrayToDataTable([
			      ['Submission Name', 'BLEU','Meteor', 'TER'],
			      ['Baseline',44.3,63.1,39.6],
			      ['CUNI_NeuralMonkeyMultimodalMT_C',49.9,67.2,34.3],
			      ['CUNI_NeuralMonkeyTextualMT_C',50.3,67.0,33.6],
			      ['DCU-ADAPT_MultiMT_C',54.1,70.1,30.0],
			      ['LIUMCVC_MNMT_C',55.9,72.1,28.4],
			      ['LIUMCVC_NMT_C',53.3,70.1,31.7],
			      ['NICT_NMTrerank_C',55.3,72.0,28.4],
			      ['OSU_1NeuralTranslation_C',51.0,67.2,33.6],
			      ['OSU_2NeuralTranslation_C',51.9,68.3,32.7],
			      ['SHEF_ShefClassInitDec_C',45.0,62.8,38.4],
			      ['SHEF_ShefClassProj_C',43.6,61.5,40.5]
				   ]);

	  // En-Fr COCO 2017
	  var data4 = google.visualization.arrayToDataTable([
			      ['Submission Name', 'BLEU','Meteor', 'TER'],
	  ['Baseline',35.1,55.8,45.8],
	  ['CUNI_NeuralMonkeyMultimodalMT_C',42.9,62.5,38.2],
	  ['CUNI_NeuralMonkeyTexutalMT_C',43.0,62.5,38.2],
	  ['DCU-ADAPT_MultiMT_C',44.5,64.1,35.2],
	  ['LIUMCVC_MNMT_C',45.9,65.9,34.2],
	  ['LIUMCVC_NMT_C',43.6,63.4,37.4],
	  ['NICT_NMTrerank_C',45.1,65.6,34.7],
	  ['OSU_1NeuralTranslation_C',41.2,61.6,37.8],
	  ['OSU_2NeuralTranslation_C',44.1,63.8,36.7],
	  ['SHEF_ShefClassInitDec_C',37.2,57.3,42.4],
	  ['SHEF_ShefClassProj_C',36.8,57.0,44.5],
				   ]);

	  // En-De Task 2 Flickr 2017
	  var data5 = google.visualization.arrayToDataTable([
	  ['Submission Name', 'BLEU','Meteor', 'TER'],
	  ['Baseline',9.1,23.4,91.4],
	  ['CMU_NeuralEncoderDecoder_C',9.1,19.8,63.3],
	  ['CUNI_NeuralMonkeyBilingual_C',2.3,17.6,112.6],
	  ['CUNI_NeuralMonkeyCaptionAndMT_C',4.2,22.1,133.6],
	  ['CUNI_NeuralMonkeyCaptionAndMT_U',6.5,20.6,91.7],
				   ]);

	  return [data, data2, data3, data4, data5]
}


function drawVisualization() {
	  // Prepare the data
	  data = return_data();
	  data1 = data[0];
	  data2 = data[1];
	  data3 = data[2];
	  data4 = data[3];
	  data5 = data[4];
      	  var cssClassNames = {
		    'headerRow': 'data-header',
		    'rowNumberCell': 'data-position'
		  	    };

	  //draw the Flick 2017 En-De data table
	  var table = new google.visualization.Table(document.getElementById('rawDataTable'));
 	  var view = new google.visualization.DataView(data1);
	  view.setColumns([0,1,2,3])
	  table.draw(view, {
		        showRowNumber: true,
		        width: '50%',
			height: '100%',
			cssClassNames: cssClassNames,
			});

	  //draw the MS COCO 2017 En-De data table
	  var table2 = new google.visualization.Table(document.getElementById('rawDataTable2'));
 	  var view2 = new google.visualization.DataView(data2);
	  view2.setColumns([0,1,2,3])
	  table2.draw(view2, {
		        showRowNumber: true,
		        width: '50%',
			height: '100%',
			cssClassNames: cssClassNames,
			});

	  //draw the Flickr 2017 En-Fr data table
	  var table3 = new google.visualization.Table(document.getElementById('rawDataTable3'));
 	  var view3 = new google.visualization.DataView(data3);
	  view3.setColumns([0,1,2,3])
	  table3.draw(view3, {
		        showRowNumber: true,
		        width: '50%',
			height: '80%',
			cssClassNames: cssClassNames,
			});

	  //draw the COCO 2017 En-Fr data table
	  var table4 = new google.visualization.Table(document.getElementById('rawDataTable4'));
 	  var view4 = new google.visualization.DataView(data4);
	  view4.setColumns([0,1,2,3])
	  table4.draw(view4, {
		        showRowNumber: true,
		        width: '50%',
			height: '80%',
			cssClassNames: cssClassNames,
			});

	  //draw the Task 2 Flickr 2017 En-De data table
	  var table5 = new google.visualization.Table(document.getElementById('rawDataTable5'));
 	  var view5 = new google.visualization.DataView(data5);
	  view5.setColumns([0,1,2,3])
	  table5.draw(view5, {
		        showRowNumber: true,
		        width: '50%',
			height: '40%',
			cssClassNames: cssClassNames,
			});
}

google.load('visualization', '1', {
	  packages: ['corechart', 'controls', 'table'],
	    callback: drawVisualization
});
</script>

<p>This is shared task is aimed at the generation of image descriptions in a target language. The task can be addressed from two different perspectives:
<ul>
<li>as a <b><a href="#task1">translation task</a></b>, which will take a source language description and translate it into the target language, where this process can be supported by information from the image (multimodal translation), and </li>
<li>as a <b><a href="#task2">description generation task</a></b>,
  which will take an image and generate a description for it in the
  target language, with <b>no</b> additional source language
  information (multilingual image description generation). </li>
</ul>

<!--We welcome participants focusing on either or both of these task variants. They differ mainly in the training data and in the way the target language descriptions are evaluated: against one translation of the corresponding source description (translation variant) or against one or more descriptions of the same image in the target language, created independently from the corresponding source description (image description variant).
-->
This shared task has the following main <b>goals</b>:

<ul>
<li>To push existing work on the integration of computer vision and language processing. </li>   
<li>To push existing work on multimodal language processing towards multilingual multimodal language processing.</li>
<li>To investigate the effectiveness of information from images in machine translation. </li>
<li>To investigate image description generation in a multilingual setting.</li>
</ul>

<p>We welcome participants focusing on either or both of these task
variants. We would also particularly like to encourage participants to consider
the unconstrained data setting for both tasks. Using additional data
is more realistic, given the small dataset sizes, but its potential 
has been previously under-explored.</p>

<h3>Important dates</h3>

    <table>
      <tr><td>Release of training data </td><td>February 8, 2017</td></tr>
      <tr><td>Release of test data </td><td>April 10 2017</td></tr>
      <tr><td>Results submission deadline  </td><td>May 12 2017</td></tr>
      <tr><td>Paper submission deadline</td><td>June 9 2017</td></tr>
      <tr><td>Notification of acceptance</td><td>June 30 2017</td></tr>
      <tr><td>Camera-ready deadline</td><td>July 14 2017</td></tr>
    </table>

<hr>
<!-- MULTIMODAL MT-->

<br>
<font color="purple">Download the <a href="http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/gold_translations_task1.tar.gz"><b>gold translations for task 1</b><a/></font>.

<h3 id="task1"><font color="blue">Task 1: Multimodal Machine Translation</font></h3>


<div>
  <img style="width: 500px; margin-left: auto; margin-right: auto; display: block;" src="figures/mmt_task1.png"></img> 
</div>

<p>This task consists in translating English sentences that describe an image into German and/or French, given the English sentence itself and the image that it describes (or features from this image, if participants chose to). For this task, the Flickr30K Entities dataset was extended in the following way: for each image, one of the English descriptions was selected and manually translated into German and French by human translators. For English-German, translations were produced by professional translators, who were given the source segment only (training set) or the source segment and image (validation and test sets). For English-French, translations were produced via crowd-sourcing where translators had access to source segment, the image and an automatic translation created with a standard phrase-based system (Moses baseline system built using the WMT'15 constrained translation task data) as a suggestion to make translation easier (note that this was <b>not</b> a post-editing task: although translators could copy and paste the suggested translation to edit, we found that they did not do so in the vast majority of cases). 
</p>

<p>As <i><font color="green">training</font></i> and <i><font color="green">development</font></i> data, we provide 29,000, and 1,014 triples respectively, each containing an English source sentence, its German and French human translations and corresponding image. We also provide the 2016 test set, which people can use for validation/evaluation. The English-German datasets are the same as those in 2016, but we note that human translations in the 2016 validation and test datasets have been post-edited (by humans) using the images to make sure the target descriptions are faithful to these images. There were cases where in the 2016 the source text was ambiguous and the image was used to solve the ambiguities. The French translations were added in 2017.  

<p>As <i><font color="green">test data</font></i>, we provide a new set of 1,000 tuples containing an English description and its corresponding image.  Gold labels will be translations in German or French. 

<p><i><font color="green">Evaluation</font></i> will be performed against the German or French human translations of  the test set using standard MT evaluation metrics, with METEOR (<a href="https://github.com/jhclark/multeval">multeval</a> implementation, but with METEOR 1.5) as the primary metric. The submissions and reference translations will be pre-processed to <a href="https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/lowercase.perl">lowercase</a>, <a href="https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/normalize-punctuation.perl">normalise punctuation</a> and <a href="https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl">tokenise</a> the sentences. Each language will be evaluated independently.
<!--
<font face="Courier New" size="2">
./multeval.sh eval --refs wmt2016/de_test/reference.ref --hyps-baseline baseline_model/de/generated --hyps-sys1 my_great_model/de_generated --meteor.language de
</font>

<p><font color="magenta">For those interested in translating in the inverse direction, i.e., German into English, we can release a test set for that direction. The training and development sets will remain the same, their translation direction can simply be flipped. </font>
-->

<p>The <i><font color="green">baselines</font></i> for this task will be neural MT systems trained using only the textual training data provided and the <a href="https://github.com/rsennrich/nematus">Nematus</a> tool (details later).
</p>

<h3 id="task1_results" style="text-align: center;"><font color="blue">En-De Flickr 2017 Results:</font></h3>
<p style="text-align: center; margin-top: -16px;">(click header to sort)</p>
<div id="rawDataTable" class="table"></div>
<h3 id="task1_results" style="text-align: center;"><font color="blue">En-De COCO 2017 Results:</font></h3>
<div id="rawDataTable2" class="table"></div>
<h3 id="task1_results" style="text-align: center;"><font color="blue">En-Fr Flickr 2017 Results:</font></h3>
<div id="rawDataTable3" class="table"></div>
<h3 id="task1_results" style="text-align: center;"><font color="blue">En-Fr COCO 2017 Results:</font></h3>
<div id="rawDataTable4" class="table"></div>

<hr>

<!-- DESCRIPTION GENERATION-->

<h3 id="task2"><font color="blue">Task 2: Multilingual Image Description Generation</font></h3>

<div>
  <img style="width: 500px; margin-left: auto; margin-right: auto; display: block;" src="figures/mmt_task2.png"></img> 
</div>

<!-- 
<p><b><font color="purple">Results <a href="http://www.quest.dcs.shef.ac.uk/wmt15_files/results/task2.pdf">here<a/></font></b>, <b>gold-standard labels</b> <a href="http://www.quest.dcs.shef.ac.uk/wmt15_files/gold/Task2_gold.tar.gz">here<a/>  
-->

<p>This task consists in generating a German sentence that describes
an image, given <b>only</b> the image for <b>unseen data</b>. For this task, the Flickr30K dataset was extended in the following way: for each image, five German descriptions were crowdsourced independently from their English versions, and independently from each other. 
Any English-German pair of descriptions for a given image could be considered a comparable translation pair.  We will provide the images and associated descriptions for training, while smaller portions will be used for development and test. 
</p>

<!-- <p><b><font color="red">Update</font></b>: -->
<!--<a href="http://staff.fnwi.uva.nl/d.elliott/wmt16/mmt_task2.zip">Download the complete release of the training and validation data</a>. This release contains 29,000 <i><font color="green">training</font></i> tuples of German described 
images and 1,014 <i><font color="green">development</font></i> tuples of German described images. A tuple contains an image paired with five (5) crowdsourced descriptions. Note, the entire English training and development datasets are included in this download. </p>-->

<p>As <i><font color="green">training</font></i> and <i><font
    color="green">development</font></i> data, we provide 29,000 and
1,014 images, each with 5 descriptions in English and 5 descriptions
in German, i.e., 29,014 tuples containing an image and 10
descriptions, 5 in each language. We also provide the 2016 test set,
which people can use for validation/evaluation.
</li>

<p>As <i><font color="green">test data</font></i>, we provide a new set of approximately 1,000 images without any English descriptions. 
<!--<a href="http://www.quest.dcs.shef.ac.uk/wmt15_files/task1_en-es_test.tar.gz">Download test data (and baseline features)</a>. 
<!--Download 17 baseline feature set for the <a href="http://www.quest.dcs.shef.ac.uk/wmt15_files/task1_en-es_test.baseline17.features">test</a> set.--></li>

<p><i><font color="green">Evaluation</font></i> will be performed against five German descriptions collected as reference on the test set, with lowercased text and without punctuation, using METEOR. We may also include manual evaluation.</p> 

<p>The <i><font color="green">baseline</font></i> for this task will
be the image description model by <a
                href="http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf">Xu
                et al. (2015)</a> trained over only the German
target language data.</p>
</p>

<h3 id="task1_results" style="text-align: center;"><font color="blue">En-De Flickr 2017 Results:</font></h3>
<div id="rawDataTable5" class="table"></div>

<hr>

<h3>Datasets</h3>

We provide training and test datasets for both variants of the task and also allow participants to use external data and resources (constrained vs unconstrained submissions). The data to be used for both tasks is an extended version of the <a href="http://shannon.cs.illinois.edu/DenotationGraph/">Flickr30K</a> dataset. The original dataset contains 31,783 images from Flickr on various topics and five crowdsourced English descriptions per image, totalling 158,915 English descriptions.

<p>Task 1: <a href="http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/mmt_task1_training.tar.gz">Training</a>, 
<a href="http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/mmt_task1_validation.tar.gz">Validation</a>, and 
<a href="http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/mmt_task1_test2016.tar.gz">2016 Test</a> sentences, and 
the <a href="https://github.com/multi30k/dataset/tree/master/data/task1/image_splits">splits</a>. 

<p>We release two new test sets for Task 1: </p>
<ul>
<li>An <i>in-domain</i> (Flickr) test set with 1,000 images. Download the <a href="http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/source_flickr.task1">source segments</a>, the <a href="http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/images_flickr.task1.tar.gz">images</a> -- which were used in this <a href="http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/image_ids_flickr.task1">order</a> -- and the <a href="http://www-lium.univ-lemans.fr/sites/default/files/NMTPY/test2017/task1_ResNet50_res4fx_test2017.fp16.npy.gz">res4_relu convolutional features</a> [260MB] and <a href="http://www-lium.univ-lemans.fr/sites/default/files/NMTPY/test2017/task1_ResNet50_pool5_test2017.mat.zip">averaged pooled features</a> [8.2MB]. 
 </li>
<li>An <i>out-of-domain</i> (MSCOCO) test set with 461 images whose captions were selected to contain ambiguous verbs. Download the <a href="http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/source_mscoco.task1">source segments</a>, the <a href="http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/images_mscoco.task1.tar.gz">images</a> -- which were used in this <a href="http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/image_ids_mscoco.task1">order</a> --  and the <a href="http://www-lium.univ-lemans.fr/sites/default/files/NMTPY/test2017/msCOCO_ResNet50_res4fx_ambiguous_dataset.fp16.npy.gz">res4_relu convolutional features<a/> [110MB] and <a href="http://www-lium.univ-lemans.fr/sites/default/files/NMTPY/test2017/msCOCO_ResNet50_pool5_ambiguous_dataset.mat.zip">averaged pooled features</a> [3.9MB]. </li>
</ul>

<p>The primary evaluation dataset will be the in-domain (Flickr) test set.


<p>Task 2: <a href="http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/mmt_task2.zip">Training and Validation</a>, and <a href="http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/mmt16_task2_test.tgz">2016 Test</a> sentences, and the <a href="https://github.com/multi30k/dataset/tree/master/data/task1/image_splits>splits</a>.


<p>We release the following new test set for Task 2: an in-domain test set with 1,071 images (note that these are different from the images in Task 1). Download the <a href="http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/images_flickr.task2.tar.gz">images</a> -- which were used in this <a href="http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/image_ids_flickr.task2">order</a> --  and the <a href="http://www-lium.univ-lemans.fr/sites/default/files/NMTPY/test2017/task2_ResNet50_res4fx_test2017.fp16.npy.gz">res4_relu convolutional features </a> [234MB] and <a href="http://www-lium.univ-lemans.fr/sites/default/files/NMTPY/test2017/task2_ResNet50_pool5_test2017.mat.zip">averaged pooled features</a> [8.8MB].</p>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:12px 7px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:12px 7px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-yw4l{vertical-align:top}
</style>
<p><b>Summary of the datasets:</b> 
<div>
<table class="tg" style="margin-left: auto; margin-right: auto; display:block;">
  <tr>
    <th class="tg-yw4l"></th>
    <th class="tg-baqh" colspan="2">Training</th>
    <th class="tg-baqh" colspan="2">Validation</th>
    <th class="tg-baqh" colspan="2">Test 2016</th>
    <th class="tg-baqh" colspan="2">Test 2017</th>
    <th class="tg-baqh" colspan="2">Ambiguous COCO</th>
  </tr>
  <tr>
    <td class="tg-031e"></td>
    <td class="tg-031e">Images</td>
    <td class="tg-031e">Sentences</td>
    <td class="tg-031e">Images</td>
    <td class="tg-yw4l">Sentences</td>
    <td class="tg-031e">Images</td>
    <td class="tg-yw4l">Sentences</td>
    <td class="tg-031e">Images</td>
    <td class="tg-yw4l">Sentences</td>
    <td class="tg-031e">Images</td>
    <td class="tg-yw4l">Sentences</td>
  </tr>
  <tr>
    <td class="tg-031e">Task 1</td>
    <td class="tg-031e" rowspan="2">29,000</td>
    <td class="tg-031e">29,000</td>
    <td class="tg-031e" rowspan="2">1,014</td>
    <td class="tg-yw4l">1,014</td>
    <td class="tg-031e" rowspan="2">1,000</td>
    <td class="tg-yw4l">1,000</td>
    <td class="tg-031e" rowspan="1">1,000</td>
    <td class="tg-yw4l">1,000</td>
    <td class="tg-yw4l">461</td>
    <td class="tg-yw4l">461</td>
  </tr>
  <tr>
    <td class="tg-031e">Task 2</td>
    <td class="tg-031e">145,000</td>
    <td class="tg-yw4l">5,070</td>
    <td class="tg-yw4l">5,000</td>
    <td class="tg-yw4l">1,071</td>
    <td class="tg-yw4l">5,355</td>
    <td class="tg-yw4l">-</td>  
    <td class="tg-yw4l">-</td>  
</tr>
</table>
</div>
</p>

<br>
<p>We also provide <b>ResNet-50 image features</b>, although their use is not mandatory. 
<ul>
<li> res4_relu convolutional features: <a href="http://www-lium.univ-lemans.fr/sites/default/files/NMTPY/transpose-fix/flickr30k_ResNets50_blck4_train.fp16.npy.gz">training [6.8GB]<a/>,  
 <a href="http://www-lium.univ-lemans.fr/sites/default/files/NMTPY/transpose-fix/flickr30k_ResNets50_blck4_val.fp16.npy.gz ">validation [242MB]</a> and  <a href="http://www-lium.univ-lemans.fr/sites/default/files/NMTPY/transpose-fix/flickr30k_ResNets50_blck4_test.fp16.npy.gz">test2016 [238MB]</a>.</li>

<li> averaged pooled features:  <a href="http://www-lium.univ-lemans.fr/sites/default/files/NMTPY/flickr30k_ResNet50_pool5_train.zip"> training [241MB]</a>, <a href="http://www-lium.univ-lemans.fr/sites/default/files/NMTPY/flickr30k_ResNet50_pool5_val.zip"> validation [8.5MB]</a> and <a href="http://www-lium.univ-lemans.fr/sites/default/files/NMTPY/flickr30k_ResNet50_pool5_test.zip">test2016 [8.3MB]</a>.</li>
</ul>
<font color="red">Note</font>: the originally distributed features had an issue with column and row inversion. Please download the corrected version of the features.

<!--<font color="magenta">VGG-19 CNN</font>, described in <a href="http://arxiv.org/abs/1409.1556">(Simonyan and Zisserman, 2015)</a> from the FC<sub>7</sub> (relu7) and CONV<sub>5_4</sub> layers using <a href="https://github.com/BVLC/caffe/releases/tag/rc2">Caffe RC2</a>.-->

<!--
<ul>
  <li>we used the matlab_features_reference code in <a href="https://github.com/karpathy/neuraltalk/tree/master/matlab_features_reference">NeuralTalk</a>
  <li>The <a href="https://staff.fnwi.uva.nl/d.elliott/wmt16/fc7_vgg_feats_hdf5-flickr30k.mat">FC_7 features</a> were extracted from the layer labelled 'relu7', as defined in the deploy_features.prototxt in NeuralTalk.</li>
  <li>The <a href="https://staff.fnwi.uva.nl/d.elliott/wmt16/conv54_vgg_feats_hdf5-flickr30k.mat">CONV_5,4 features</a> (warning: 11GB file) were extracted from the layer labelled 'conv5_4', following correspondence with Kelvin Xu.</li>
  <li>For those who want to extract other image features, the original images can be downloaded from the <a href="http://shannon.cs.illinois.edu/DenotationGraph/">Flickr30K</a> dataset.</li>
</ul>
-->

<p>If you use the dataset created for this shared task, please cite the following paper: <a href="http://aclweb.org/anthology/W16-3210.pdf">Multi30K: Multilingual English-German Image Descriptions</a>.

<p>
<pre>
@inproceedings{elliott-EtAl:2016:VL16,
 author    = {{Elliott}, D. and {Frank}, S. and {Sima'an}, K. and {Specia}, L.},
 title     = {Multi30K: Multilingual English-German Image Descriptions},
 booktitle = {Proceedings of the 5th Workshop on Vision and Language},
 year      = {2016},
 pages     = {70--74},
 year      = 2016
}
</pre>
</p>

<p>If you use the Test 2017 or the Ambiguous COCO evaluation data, which were created for this shared task, please cite the following paper:

<p>
<pre>
@inproceedings{ElliottFrankBarraultBougaresSpecia2017,
 author = {Desmond Elliott and Stella Frank and Lo\"{i}c Barrault and Fethi Bougares and Lucia Specia},
 title = {{Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description}},
 booktitle = {Proceedings of the Second Conference on Machine Translation},
 year = {2017},
 month = {September},
 address = {Copenhagen, Denmark}
}
</pre>
</p>


<hr>

<!-- EXTRA STUFF -->

<h3>Additional resources</h3>

<p>We suggest the following <b><font color="green">interesting resources</font></b> that can be used as additional training data for either or both tasks:
<ul>
<li><a href="http://www.statmt.org/wmt17/translation-task.html">WMT17 News translation task data</a> for both bilingual (English-German) and monolingual (English or German) data.</li>
<li><a href="http://www.statmt.org/wmt15/translation-task.html">WMT15 News translation task data</a> for both bilingual (English-French) and monolingual (English or French) data.</li>
<li><a href="http://statmt.org/rsennrich/wmt16_systems/">Pre-trained Nematus models (on News)</a> for (English-German).</li>
<li><a href="http://web.engr.illinois.edu/~bplumme2/Flickr30kEntities/">Flickr30K Entities</a> dataset: an extension of the Flickr30K dataset which contains additional layers of annotation such as 244K coreference chains in the English descriptions and 276K manually annotated bounding boxes for entities in the images.</li>
<li>Additional image description datasets for the English models, such as the <a href="http://mscoco.org/">Microsoft COCO Dataset</a>, among others. See <a href="http://visionandlanguage.net/">this survey</a> for a complete list.
</ul>

Submissions using these or any other resources external to those provided for the tasks should indicate that their submissions are of the "unconstrained" type. 

<hr>

<!-- SUBMISSION INFO -->

<h3>Submission Requirements</h3>

<p>You are encouraged to submit a short report (4 to 6 pages) to WMT
describing your method(s). You are not required to
submit a paper if you do not want to. In that case, we ask you
to provide a summary and/or an appropriate reference describing your method(s) that we can cite
in the WMT overview paper.</p>

Each participating team can submit at most 2 systems for each of the task variants (so up to 4 submissions). These should be sent
via email to Lucia Specia <a href="mailto:lspecia@gmail.com" target="_blank">lspecia@gmail.com</a>. Please use the following pattern to name your files:
<p>
<code>INSTITUTION-NAME</code>_<code>TASK-NAME</code>_<code>METHOD-NAME</code>_<code>TYPE</code>, where:
<p> <code>INSTITUTION-NAME</code> is an acronym/short name for your institution, e.g. SHEF
<p><code>TASK-NAME</code> is one of the following: 1 (translation), 2 (description), 3 (both).
<p><code>METHOD-NAME</code> is an identifier for your method in case you have multiple methods for the same task, e.g. 2_NeuralTranslation, 2_Moses
<p><code>TYPE</code> is either C or U, where C indicates "constrained", i.e. using only the resources provided by the task organisers, and U indicates "unconstrained".
<p> For instance, a constrained submission from team SHEF for task 2 using method "Moses" could be named SHEF_2_Moses_C.
<p>If you are submitting a system for Task 1, please include the dataset and language in the <code>TASK</code> tag, e.g. 1_FLICKR_DE, 1_COCO_FR, etc.</p>
  
<h3>Submission Format</h3>

<p> The output of your system <b>a given task</b> should produce a target language description for each image formatted in the following way: </p>
<pre>&lt;METHOD NAME&gt; &lt;IMAGE ID&gt; &lt;DESCRIPTION&gt; &lt;TASK&gt; &lt;TYPE&gt;<br><br></pre>
Where:
<ul>
<li><code>METHOD NAME</code> is the name of your method.</li>
<li><code>IMAGE ID</code> is the identifier of the test image.</li>
<li><code>DESCRIPTION</code> is the output generated by your system (either a translation or an independently generated description). </li>
<li><code>TASK</code> is one of the following flags: 1 (for translation task), 2 (for image description task), 3 (for both). The choice here will indicate how your descriptions will be evaluated. Option 3 means they will be evaluated both as a translation task and as an image description task.</li>
<li><code>TYPE</code> is either C or U, where C indicates "constrained", i.e. using only the resources provided by the task organisers, and U indicates "unconstrained".</li>
</ul>
Each field should be delimited by a single tab character.

<h3>Organisers</h3>
Lucia Specia (University of Sheffield)
<br>
Stella Frank (University of Amsterdam)
<br>
Lo&iuml;c Barrault (University of Le Mans)
<br>
Fethi Bougares (University of Le Mans)
<br>
Desmond Elliott (University of Amsterdam)

<h3>Contact</h3>
<p> For questions or comments, email Lucia
Specia <a href="mailto:lspecia@gmail.com" target="_blank">lspecia@gmail.com</a>.
</p>

<h3>License</h3>
The data is licensed under Creative Commons: <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Attribution-NonCommercial-ShareAlike 4.0 International</a>.  

<br><br>

<p align="right">
Supported by the European Commission under the
<a href="http://cordis.europa.eu/project/rcn/200709_en.html"> MultiMT <img align=right src="figures/ERC_Logo.png" border=0 width=100 height=40></a> 
and  <a href="http://m2cr.univ-lemans.fr/"> M2CR <img align=right src="figures/m2cr_logo.png" border=0 width=100 height=40></a>
<br>projects <p>


</body></html>
